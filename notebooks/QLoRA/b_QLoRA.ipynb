{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/volume/LegalEaseMaxim/CPSC5830-Team1/notebooks/QLoRA/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Read Token: None\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import warnings\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "PUBLISH_TO_HUB = False\n",
    "env_path = \"/media/volume/LegalEase/Repos/CPSC5830-Team1/.env\"\n",
    "load_dotenv(env_path)\n",
    "HF_READ_TOKEN = os.getenv(\"BENS_HUGGING_FACE_READ_TOKEN\")\n",
    "HF_WRITE_TOKEN = os.getenv(\"BENS_HUGGING_FACE_WRITE_TOKEN\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "print(f\"Read Token: {HF_READ_TOKEN}\")  # Ensure this is not None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.34s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    torch_dtype=torch.bfloat16,  \n",
    "    device_map=\"auto\",\n",
    "    use_auth_token=HF_READ_TOKEN,\n",
    "    cache_dir=\"/media/volume/LegalEaseMaxim/cache\"  # Ensure this path exists\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacity of 39.38 GiB of which 122.25 MiB is free. Process 427695 has 27.52 GiB memory in use. Including non-PyTorch memory, this process has 11.72 GiB memory in use. Of the allocated memory 10.94 GiB is allocated by PyTorch, and 303.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39mgradient_checkpointing_enable()\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_model_for_kbit_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/volume/LegalEaseMaxim/CPSC5830-Team1/notebooks/QLoRA/venv/lib/python3.11/site-packages/peft/utils/other.py:140\u001b[0m, in \u001b[0;36mprepare_model_for_kbit_training\u001b[0;34m(model, use_gradient_checkpointing, gradient_checkpointing_kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters():\n\u001b[1;32m    137\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    138\u001b[0m             (param\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16) \u001b[38;5;129;01mor\u001b[39;00m (param\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mbfloat16)\n\u001b[1;32m    139\u001b[0m         ) \u001b[38;5;129;01mand\u001b[39;00m param\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams4bit\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 140\u001b[0m             param\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    143\u001b[0m     loaded_in_kbit\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m is_gptq_quantized\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    149\u001b[0m ) \u001b[38;5;129;01mand\u001b[39;00m use_gradient_checkpointing:\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;66;03m# When having `use_reentrant=False` + gradient_checkpointing, there is no need for this hack\u001b[39;00m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_reentrant\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m gradient_checkpointing_kwargs \u001b[38;5;129;01mor\u001b[39;00m gradient_checkpointing_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_reentrant\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;66;03m# For backward compatibility\u001b[39;00m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacity of 39.38 GiB of which 122.25 MiB is free. Process 427695 has 27.52 GiB memory in use. Including non-PyTorch memory, this process has 11.72 GiB memory in use. Of the allocated memory 10.94 GiB is allocated by PyTorch, and 303.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=\"/media/volume/LegalEaseMaxim/cache\")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Ensure padding is set correctly\n",
    "tokenizer.padding_side = \"left\"\n",
    "#Load Dataset (Structured Chat Data Format)\n",
    "dataset_path = {\"train\": \"my_dataset.json\"}  # Ensures correct train key\n",
    "data = load_dataset(\"json\", data_files=dataset_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 92/92 [00:00<00:00, 7183.36 examples/s]\n",
      "Map: 100%|██████████| 11/11 [00:00<00:00, 2596.07 examples/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Split Train and Test\n",
    "split_data = data[\"train\"].train_test_split(test_size=0.1)\n",
    "def format_prompt(example):\n",
    "    messages = example[\"messages\"]\n",
    "    formatted_text = \"\"\n",
    "    for msg in messages:\n",
    "        role = msg[\"role\"]\n",
    "        content = msg[\"content\"]\n",
    "        if role == \"system\":\n",
    "            formatted_text += f\"[SYSTEM] {content} [/SYSTEM]\\n\"\n",
    "        elif role == \"user\":\n",
    "            formatted_text += f\"[INST] {content} [/INST]\\n\"\n",
    "        elif role == \"assistant\":\n",
    "            formatted_text += f\"{content}\\n\"\n",
    "    return {\"formatted_text\": formatted_text}\n",
    "formatted_data = split_data.map(format_prompt).remove_columns([\"messages\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"formatted_text\"], truncation=True, padding=\"max_length\", max_length=1024, add_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 92/92 [00:00<00:00, 2752.17 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Map: 100%|██████████| 11/11 [00:00<00:00, 1267.20 examples/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenized_data = formatted_data.map(tokenize_function, batched=True)\n",
    "config = LoraConfig(\n",
    "    r=16, \n",
    "    lora_alpha=32, \n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05, \n",
    "    bias=\"none\", \n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 13,631,488 || all params: 7,255,363,584 || trainable%: 0.1879\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters() \n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/media/volume/LegalEaseMaxim/output\",  # Store model checkpoints on volume\n",
    "    learning_rate=2e-4,  # Adjusted for Stability\n",
    "    per_device_train_batch_size=4,  # Adjust Based on Available VRAM\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=5,  # 5 Epochs is Sufficient for 600 Pairs\n",
    "    weight_decay=0.01,\n",
    "    logging_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    gradient_accumulation_steps=4,  # Helps with Large Batch Sizes\n",
    "    warmup_steps=30,  #Dynamic Warmup (Shorter for Small Dataset)\n",
    "    fp16=False,  \n",
    "    bf16=True,#Use BF16 Instead for A100\n",
    "    optim=\"paged_adamw_8bit\"  # Optimized for Large Models\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_data[\"train\"],\n",
    "    eval_dataset=tokenized_data[\"test\"],\n",
    "    args=training_args,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 02:16, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.868000</td>\n",
       "      <td>3.169191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.544600</td>\n",
       "      <td>1.586795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.124600</td>\n",
       "      <td>0.656691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.370100</td>\n",
       "      <td>0.453396</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "adapter_model.safetensors: 100%|██████████| 54.6M/54.6M [00:01<00:00, 30.8MB/s]\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model successfully uploaded to: https://huggingface.co/XCIT3D247/LegalEaseV2\n"
     ]
    }
   ],
   "source": [
    "model.config.use_cache = False \n",
    "trainer.train()\n",
    "model.config.use_cache = True \n",
    "model.save_pretrained(\"./business_llm\")\n",
    "tokenizer.save_pretrained(\"./business_llm\")\n",
    "model.cpu()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "PUBLISH_TO_HUB = True\n",
    "if PUBLISH_TO_HUB:\n",
    "    repo_name = \"XCIT3D247/LegalEaseV2\"\n",
    "    model.push_to_hub(repo_name, use_auth_token=HF_WRITE_TOKEN)\n",
    "    tokenizer.push_to_hub(repo_name, use_auth_token=HF_WRITE_TOKEN)\n",
    "    print(f\"Model successfully uploaded to: https://huggingface.co/{repo_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.cpu()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.08it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the repo name where the model is stored\n",
    "repo_name = \"XCIT3D247/LegalEaseV2\"\n",
    "\n",
    "# Load the tokenizer and model from Hugging Face Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained(repo_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(repo_name, torch_dtype=torch.bfloat16, device_map=\"auto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to generate responses\n",
    "def generate_response(prompt, max_length=1028):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs, max_length=max_length, pad_token_id=tokenizer.eos_token_id)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test the model with a few questions\n",
    "test_questions = [\n",
    "    \"What type of business entity should I choose for a tech startup?\",\n",
    "    \"What are the tax implications of forming an LLC?\",\n",
    "    \"How does Delaware compare to Washington for incorporating a business?\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What type of business entity should I choose for a tech startup?\n",
      "A: What type of business entity should I choose for a tech startup?\n",
      "\n",
      "A Delaware C-Corporation is a popular choice for tech startups due to its flexibility, ability to issue stock, and favorable corporate tax laws. However, an LLC or an S-Corporation may also be suitable depending on the specific needs of the business.\n",
      "\n",
      "What are the benefits of a Delaware C-Corporation for a tech startup?\n",
      "\n",
      "A Delaware C-Corporation offers several benefits for a tech startup, including:\n",
      "\n",
      "- Flexibility: A C-Corporation can issue different classes of stock, allowing founders to retain control while attracting investors.\n",
      "- Limited Liability: Shareholders are not personally liable for corporate debts or liabilities.\n",
      "- Tax Advantages: C-Corporations can deduct losses and depreciation, reducing taxable income.\n",
      "- Attracting Investors: C-Corporations can issue preferred stock, which can be attractive to investors seeking dividends and liquidation preferences.\n",
      "\n",
      "What are the disadvantages of a Delaware C-Corporation for a tech startup?\n",
      "\n",
      "A Delaware C-Corporation may have the following disadvantages for a tech startup:\n",
      "\n",
      "- Complexity: C-Corporations have more formalities, such as holding annual meetings and maintaining corporate records.\n",
      "- Double Taxation: Corporate profits are taxed at the corporate level, and then again when distributed as dividends to shareholders.\n",
      "- Costs: There are filing fees and ongoing compliance costs associated with maintaining a C-Corporation.\n",
      "\n",
      "What are the benefits of an LLC for a tech startup?\n",
      "\n",
      "An LLC offers several benefits for a tech startup, including:\n",
      "\n",
      "- Flexibility: An LLC can be managed by its members or by hired managers, allowing for operational flexibility.\n",
      "- Pass-Through Taxation: LLC profits are not subject to corporate taxation; instead, they are taxed as personal income for the members.\n",
      "- Liability Protection: Members are not personally liable for business debts or liabilities.\n",
      "- Easy to Form: LLCs are easier and less expensive to form than C-Corporations.\n",
      "\n",
      "What are the disadvantages of an LLC for a tech startup?\n",
      "\n",
      "An LLC may have the following disadvantages for a tech startup:\n",
      "\n",
      "- Limited Flexibility: An LLC cannot issue different classes of stock, making it less attractive to investors seeking equity stakes.\n",
      "- Lack of Corporate Structure: An LLC does not have the same formal structure as a C-Corporation, which may be a disadvantage for larger, more complex businesses.\n",
      "\n",
      "What are the benefits of an S-Corporation for a tech startup?\n",
      "\n",
      "An S-Corporation offers several benefits for a tech startup, including:\n",
      "\n",
      "- Pass-Through Taxation: S-Corporation profits are not subject to corporate taxation; instead, they are taxed as personal income for the shareholders.\n",
      "- Flexibility: S-Corporations can issue stock, but they are limited to 100 shareholders and must be U.S. citizens or resident aliens.\n",
      "- Liability Protection: Shareholders are not personally liable for business debts or liabilities.\n",
      "- Corporate Structure: S-Corporations have a formal corporate structure, which may be beneficial for larger, more complex businesses.\n",
      "\n",
      "What are the disadvantages of an S-Corporation for a tech startup?\n",
      "\n",
      "An S-Corporation may have the following disadvantages for a tech startup:\n",
      "\n",
      "- Limited Flexibility: S-Corporations cannot issue preferred stock or have multiple classes of stock, making it less attractive to investors seeking equity stakes.\n",
      "- Complexity: S-Corporations have more formalities, such as holding annual meetings and maintaining corporate records.\n",
      "- Ownership Restrictions: S-Corporations are limited to 100 shareholders, and all must consent to certain corporate actions.\n",
      "\n",
      "What are the key considerations when choosing a business entity for a tech startup?\n",
      "\n",
      "When choosing a business entity for a tech startup, consider the following key factors:\n",
      "\n",
      "- Funding Needs: Determine whether you need to issue stock to attract investors or if an LLC or S-Corporation is sufficient.\n",
      "- Taxation: Consider the tax implications of each entity type and how they align with your business goals.\n",
      "- Liability Protection: Ensure that the entity provides adequate liability protection for the founders and investors.\n",
      "- Operational Flexibility: Consider the operational requirements of your business and how each entity type can accommodate those needs.\n",
      "- Costs and Complexity: Evaluate the costs and ongoing compliance requirements of each entity type and determine which is\n",
      "\n",
      "Q: What are the tax implications of forming an LLC?\n",
      "A: What are the tax implications of forming an LLC?\n",
      "\n",
      "An LLC is a pass-through entity, meaning that the business itself does not pay taxes. Instead, the LLC’s profits and losses are passed through to the owners, who report them on their personal tax returns.\n",
      "\n",
      "However, the LLC may be required to file an informational tax return (Form 1120S) to report income, deductions, and other information to the IRS and shareholders.\n",
      "\n",
      "Additionally, LLC members may be subject to self-employment taxes on their share of the LLC’s profits.\n",
      "\n",
      "It’s important to consult with a tax professional to understand the specific tax implications of forming an LLC and to ensure compliance with all applicable tax laws.\n",
      "\n",
      "Q: How does Delaware compare to Washington for incorporating a business?\n",
      "A: How does Delaware compare to Washington for incorporating a business?\n",
      "\n",
      "Delaware is a popular choice for incorporating a business due to its business-friendly laws, including flexible corporate structures, no minimum capital requirements, and low annual fees. However, Washington State also offers several advantages, such as no corporate income tax, no franchise tax, and a strong economy.\n",
      "\n",
      "When deciding between Delaware and Washington, consider the following factors:\n",
      "\n",
      "1. Taxation: Delaware imposes a corporate income tax, while Washington does not. However, Washington collects business and occupation taxes on net income.\n",
      "2. Annual Fees: Delaware charges an annual franchise tax of $350, while Washington does not have a franchise tax.\n",
      "3. Business Structure: Delaware offers more flexible corporate structures, including the ability to elect S-Corporation status, while Washington requires a specific business structure.\n",
      "4. Privacy: Delaware offers greater privacy, as it does not require the disclosure of corporate officers or directors. Washington, on the other hand, requires the filing of an annual report with the Secretary of State, which includes the names and addresses of officers and directors.\n",
      "5. Jurisdiction: If you plan to operate your business primarily in Washington, it may be more convenient to incorporate there, as you will not need to file foreign qualifications.\n",
      "\n",
      "Ultimately, the choice between Delaware and Washington depends on your specific business needs, including taxation, privacy, and jurisdiction. Consult with a business attorney or accountant to help you make the best decision.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for question in test_questions:\n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"A: {generate_response(question)}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
