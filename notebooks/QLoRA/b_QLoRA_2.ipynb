{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bphilipose/miniconda3/envs/LegalEaseWSL/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from peft import prepare_model_for_kbit_training\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "#have note book ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA extension not installed.\n",
      "CUDA extension not installed.\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
      "Some weights of the model checkpoint at TheBloke/Mistral-7B-Instruct-v0.2-GPTQ were not used when initializing MistralForCausalLM: {'model.layers.17.self_attn.o_proj.bias', 'model.layers.3.mlp.up_proj.bias', 'model.layers.14.mlp.gate_proj.bias', 'model.layers.30.self_attn.k_proj.bias', 'model.layers.11.self_attn.q_proj.bias', 'model.layers.13.mlp.down_proj.bias', 'model.layers.14.self_attn.k_proj.bias', 'model.layers.26.mlp.down_proj.bias', 'model.layers.4.mlp.up_proj.bias', 'model.layers.9.mlp.down_proj.bias', 'model.layers.18.self_attn.v_proj.bias', 'model.layers.31.self_attn.v_proj.bias', 'model.layers.18.mlp.gate_proj.bias', 'model.layers.2.mlp.gate_proj.bias', 'model.layers.21.self_attn.q_proj.bias', 'model.layers.4.self_attn.k_proj.bias', 'model.layers.0.self_attn.o_proj.bias', 'model.layers.5.self_attn.q_proj.bias', 'model.layers.19.mlp.down_proj.bias', 'model.layers.27.mlp.gate_proj.bias', 'model.layers.20.self_attn.k_proj.bias', 'model.layers.28.mlp.down_proj.bias', 'model.layers.9.self_attn.q_proj.bias', 'model.layers.1.self_attn.k_proj.bias', 'model.layers.12.self_attn.k_proj.bias', 'model.layers.21.self_attn.o_proj.bias', 'model.layers.5.mlp.up_proj.bias', 'model.layers.2.self_attn.o_proj.bias', 'model.layers.26.mlp.up_proj.bias', 'model.layers.24.mlp.down_proj.bias', 'model.layers.29.mlp.down_proj.bias', 'model.layers.20.mlp.up_proj.bias', 'model.layers.1.self_attn.q_proj.bias', 'model.layers.20.mlp.down_proj.bias', 'model.layers.30.mlp.up_proj.bias', 'model.layers.10.mlp.down_proj.bias', 'model.layers.3.mlp.down_proj.bias', 'model.layers.7.mlp.down_proj.bias', 'model.layers.5.self_attn.k_proj.bias', 'model.layers.3.self_attn.o_proj.bias', 'model.layers.23.self_attn.q_proj.bias', 'model.layers.7.mlp.up_proj.bias', 'model.layers.16.mlp.gate_proj.bias', 'model.layers.27.self_attn.o_proj.bias', 'model.layers.30.mlp.gate_proj.bias', 'model.layers.29.mlp.up_proj.bias', 'model.layers.12.self_attn.q_proj.bias', 'model.layers.11.mlp.down_proj.bias', 'model.layers.8.self_attn.q_proj.bias', 'model.layers.2.self_attn.v_proj.bias', 'model.layers.15.self_attn.v_proj.bias', 'model.layers.21.self_attn.v_proj.bias', 'model.layers.24.self_attn.o_proj.bias', 'model.layers.13.self_attn.v_proj.bias', 'model.layers.13.mlp.gate_proj.bias', 'model.layers.26.self_attn.v_proj.bias', 'model.layers.28.self_attn.q_proj.bias', 'model.layers.15.mlp.up_proj.bias', 'model.layers.9.self_attn.k_proj.bias', 'model.layers.19.self_attn.o_proj.bias', 'model.layers.23.self_attn.o_proj.bias', 'model.layers.27.self_attn.k_proj.bias', 'model.layers.22.self_attn.v_proj.bias', 'model.layers.10.mlp.up_proj.bias', 'model.layers.22.mlp.down_proj.bias', 'model.layers.24.mlp.gate_proj.bias', 'model.layers.31.self_attn.k_proj.bias', 'model.layers.0.self_attn.q_proj.bias', 'model.layers.9.self_attn.o_proj.bias', 'model.layers.5.self_attn.o_proj.bias', 'model.layers.12.mlp.down_proj.bias', 'model.layers.16.self_attn.v_proj.bias', 'model.layers.2.self_attn.k_proj.bias', 'model.layers.23.self_attn.k_proj.bias', 'model.layers.15.mlp.gate_proj.bias', 'model.layers.3.self_attn.v_proj.bias', 'model.layers.11.self_attn.o_proj.bias', 'model.layers.6.self_attn.o_proj.bias', 'model.layers.26.self_attn.o_proj.bias', 'model.layers.7.self_attn.k_proj.bias', 'model.layers.18.mlp.down_proj.bias', 'model.layers.19.self_attn.v_proj.bias', 'model.layers.26.mlp.gate_proj.bias', 'model.layers.18.self_attn.k_proj.bias', 'model.layers.13.self_attn.q_proj.bias', 'model.layers.25.self_attn.o_proj.bias', 'model.layers.20.mlp.gate_proj.bias', 'model.layers.11.self_attn.k_proj.bias', 'model.layers.8.mlp.down_proj.bias', 'model.layers.1.self_attn.v_proj.bias', 'model.layers.4.self_attn.q_proj.bias', 'model.layers.4.self_attn.o_proj.bias', 'model.layers.21.mlp.up_proj.bias', 'model.layers.11.mlp.up_proj.bias', 'model.layers.20.self_attn.o_proj.bias', 'model.layers.22.mlp.up_proj.bias', 'model.layers.23.mlp.up_proj.bias', 'model.layers.10.self_attn.v_proj.bias', 'model.layers.20.self_attn.v_proj.bias', 'model.layers.27.self_attn.v_proj.bias', 'model.layers.29.self_attn.v_proj.bias', 'model.layers.9.self_attn.v_proj.bias', 'model.layers.18.mlp.up_proj.bias', 'model.layers.31.mlp.gate_proj.bias', 'model.layers.31.mlp.down_proj.bias', 'model.layers.22.mlp.gate_proj.bias', 'model.layers.22.self_attn.k_proj.bias', 'model.layers.14.mlp.up_proj.bias', 'model.layers.14.mlp.down_proj.bias', 'model.layers.4.mlp.down_proj.bias', 'model.layers.6.self_attn.q_proj.bias', 'model.layers.6.self_attn.v_proj.bias', 'model.layers.9.mlp.gate_proj.bias', 'model.layers.28.mlp.up_proj.bias', 'model.layers.22.self_attn.q_proj.bias', 'model.layers.11.self_attn.v_proj.bias', 'model.layers.18.self_attn.o_proj.bias', 'model.layers.7.self_attn.v_proj.bias', 'model.layers.16.self_attn.o_proj.bias', 'model.layers.27.mlp.up_proj.bias', 'model.layers.13.self_attn.k_proj.bias', 'model.layers.24.mlp.up_proj.bias', 'model.layers.25.mlp.gate_proj.bias', 'model.layers.9.mlp.up_proj.bias', 'model.layers.17.self_attn.v_proj.bias', 'model.layers.15.self_attn.k_proj.bias', 'model.layers.8.self_attn.k_proj.bias', 'model.layers.19.self_attn.k_proj.bias', 'model.layers.3.self_attn.k_proj.bias', 'model.layers.1.mlp.down_proj.bias', 'model.layers.5.mlp.gate_proj.bias', 'model.layers.6.mlp.gate_proj.bias', 'model.layers.0.mlp.down_proj.bias', 'model.layers.15.mlp.down_proj.bias', 'model.layers.0.self_attn.v_proj.bias', 'model.layers.4.mlp.gate_proj.bias', 'model.layers.30.self_attn.o_proj.bias', 'model.layers.19.mlp.gate_proj.bias', 'model.layers.23.mlp.gate_proj.bias', 'model.layers.27.self_attn.q_proj.bias', 'model.layers.17.self_attn.q_proj.bias', 'model.layers.15.self_attn.o_proj.bias', 'model.layers.24.self_attn.q_proj.bias', 'model.layers.12.mlp.up_proj.bias', 'model.layers.10.self_attn.o_proj.bias', 'model.layers.28.self_attn.o_proj.bias', 'model.layers.3.self_attn.q_proj.bias', 'model.layers.0.self_attn.k_proj.bias', 'model.layers.7.mlp.gate_proj.bias', 'model.layers.23.mlp.down_proj.bias', 'model.layers.8.mlp.gate_proj.bias', 'model.layers.0.mlp.up_proj.bias', 'model.layers.31.mlp.up_proj.bias', 'model.layers.7.self_attn.o_proj.bias', 'model.layers.14.self_attn.o_proj.bias', 'model.layers.21.mlp.down_proj.bias', 'model.layers.1.mlp.gate_proj.bias', 'model.layers.2.mlp.down_proj.bias', 'model.layers.25.mlp.down_proj.bias', 'model.layers.10.self_attn.q_proj.bias', 'model.layers.25.mlp.up_proj.bias', 'model.layers.19.self_attn.q_proj.bias', 'model.layers.27.mlp.down_proj.bias', 'model.layers.21.self_attn.k_proj.bias', 'model.layers.7.self_attn.q_proj.bias', 'model.layers.29.self_attn.q_proj.bias', 'model.layers.10.mlp.gate_proj.bias', 'model.layers.3.mlp.gate_proj.bias', 'model.layers.22.self_attn.o_proj.bias', 'model.layers.29.mlp.gate_proj.bias', 'model.layers.12.self_attn.v_proj.bias', 'model.layers.12.self_attn.o_proj.bias', 'model.layers.17.mlp.gate_proj.bias', 'model.layers.17.mlp.down_proj.bias', 'model.layers.2.self_attn.q_proj.bias', 'model.layers.5.mlp.down_proj.bias', 'model.layers.6.mlp.up_proj.bias', 'model.layers.13.mlp.up_proj.bias', 'model.layers.28.mlp.gate_proj.bias', 'model.layers.30.self_attn.q_proj.bias', 'model.layers.6.mlp.down_proj.bias', 'model.layers.4.self_attn.v_proj.bias', 'model.layers.19.mlp.up_proj.bias', 'model.layers.5.self_attn.v_proj.bias', 'model.layers.16.mlp.down_proj.bias', 'model.layers.30.self_attn.v_proj.bias', 'model.layers.24.self_attn.v_proj.bias', 'model.layers.6.self_attn.k_proj.bias', 'model.layers.2.mlp.up_proj.bias', 'model.layers.28.self_attn.k_proj.bias', 'model.layers.21.mlp.gate_proj.bias', 'model.layers.25.self_attn.q_proj.bias', 'model.layers.23.self_attn.v_proj.bias', 'model.layers.11.mlp.gate_proj.bias', 'model.layers.8.self_attn.v_proj.bias', 'model.layers.16.mlp.up_proj.bias', 'model.layers.25.self_attn.v_proj.bias', 'model.layers.12.mlp.gate_proj.bias', 'model.layers.29.self_attn.k_proj.bias', 'model.layers.26.self_attn.k_proj.bias', 'model.layers.10.self_attn.k_proj.bias', 'model.layers.18.self_attn.q_proj.bias', 'model.layers.31.self_attn.o_proj.bias', 'model.layers.8.mlp.up_proj.bias', 'model.layers.14.self_attn.v_proj.bias', 'model.layers.25.self_attn.k_proj.bias', 'model.layers.8.self_attn.o_proj.bias', 'model.layers.17.mlp.up_proj.bias', 'model.layers.30.mlp.down_proj.bias', 'model.layers.29.self_attn.o_proj.bias', 'model.layers.0.mlp.gate_proj.bias', 'model.layers.31.self_attn.q_proj.bias', 'model.layers.26.self_attn.q_proj.bias', 'model.layers.28.self_attn.v_proj.bias', 'model.layers.1.self_attn.o_proj.bias', 'model.layers.20.self_attn.q_proj.bias', 'model.layers.14.self_attn.q_proj.bias', 'model.layers.13.self_attn.o_proj.bias', 'model.layers.15.self_attn.q_proj.bias', 'model.layers.16.self_attn.q_proj.bias', 'model.layers.16.self_attn.k_proj.bias', 'model.layers.24.self_attn.k_proj.bias', 'model.layers.17.self_attn.k_proj.bias', 'model.layers.1.mlp.up_proj.bias'}\n",
      "- This IS expected if you are initializing MistralForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MistralForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = \"TheBloke/Mistral-7B-Instruct-v0.2-GPTQ\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map={\"\": 0}, trust_remote_code=False, revision=\"main\")\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Apply tokenization\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment = \"I want to start a foodbank in Washington, how do I?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt with instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions_string = f\"\"\"You are an expert business consultant designed to help users determine the most suitable business structure for their needs. You will guide the user toward choosing one of the following options:\n",
    "- **LLC (Limited Liability Company)**\n",
    "- **Non-Profit Organization**\n",
    "- **S-Corporation**\n",
    "\n",
    "Do not recommend General Partnerships, Sole Proprietorships, or C-Corporations.\n",
    "\n",
    "### **Your Approach**\n",
    "1. **Ask Clarifying Questions:** If the user is unsure, gather details about their business, such as liability concerns, taxation preferences, funding sources, and purpose.\n",
    "2. **Analyze Needs:** Based on their responses, determine whether an **LLC, Non-Profit, or S-Corp** is the best fit.\n",
    "3. **Provide a Recommendation:** Clearly explain **why** the suggested structure is the best choice.\n",
    "4. **Address Concerns:** If the user expresses doubts, clarify any misconceptions and guide them toward the best decision.\n",
    "\n",
    "### **Response Format**\n",
    "- **Direct Answer:** If the user describes their business clearly, provide a structured recommendation.\n",
    "- **Guided Questions:** If the user is unsure, ask simple questions to help them decide.\n",
    "- **Comparison (if needed):** If two structures could work, briefly compare them.\n",
    "\n",
    "### **Example Scenarios**\n",
    "#### **User Input 1:**  \n",
    "*\"I’m starting a consulting business with a partner. We want liability protection and pass-through taxation.\"*  \n",
    "**LLM Response:**  \n",
    "*\"An LLC would be ideal for you. It offers **liability protection**, meaning your personal assets are safe, and it allows **pass-through taxation**, so profits aren’t taxed twice. You’ll also have flexibility in management and ownership. Would you like help with the next steps?\"*\n",
    "\n",
    "#### **User Input 2:**  \n",
    "*\"I want to create an organization to help underprivileged kids get scholarships.\"*  \n",
    "**LLM Response:**  \n",
    "*\"A Non-Profit is the best choice for your mission. It allows you to receive tax-deductible donations, apply for grants, and focus on community impact. To qualify, your organization must serve the public interest. Do you need help with 501(c)(3) registration?\"*\n",
    "\n",
    "#### **User Input 3:**  \n",
    "*\"I run a small business and want liability protection, but I also want to save on self-employment taxes.\"*  \n",
    "**LLM Response:**  \n",
    "*\"An S-Corporation might be your best option. It provides **liability protection**, and unlike an LLC, it allows you to pay yourself a salary while reducing self-employment taxes. Would you like a quick breakdown of how S-Corp taxation works?\"*\n",
    "\n",
    "**Now, respond to the following user inquiry:**\n",
    "\"\"\"\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "     \n",
    "#prompt=f'''[INST] {comment} [/INST]'''\n",
    "\n",
    "prompt_template = lambda in_comment: f'''[INST] {instructions_string} \\n{in_comment} \\n[/INST]'''\n",
    "prompt = prompt_template(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "#generate output\n",
    "outputs = model.generate(input_ids=inputs[\"input_ids\"].to('cuda'), max_new_tokens=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " **LLM Response:**\n",
      "\n",
      "Starting a food bank in Washington involves several steps to establish a non-profit organization. Here's a simplified guide:\n",
      "\n",
      "1. **Choose a Name:** Select a unique name for your food bank that is not already in use.\n",
      "2. **Create a Board of Directors:** Assemble a board of directors to oversee the organization's operations.\n",
      "3. **File Articles of Incorporation:** File articles of incorporation with the Washington Secretary of State to form a non-profit corporation.\n",
      "4. **Apply for Tax-Exempt Status:** Apply for tax-exempt status from the Internal Revenue Service (IRS) as a 501(c)(3) organization.\n",
      "5. **Register with the Washington State Department of Revenue:** Register with the Washington State Department of Revenue to collect and report sales tax.\n",
      "6. **Obtain Necessary Permits and Licenses:** Obtain any necessary permits and licenses, such as a food service permit, from local and state agencies.\n",
      "7. **Set Up a Bank Account:** Set up a bank account for your non-profit organization.\n",
      "8. **Create Bylaws and Policies:** Develop bylaws, policies, and procedures for your food bank.\n",
      "9. **Raise Funds and Collect Donations:** Begin raising funds and collecting donations to support your food bank's mission.\n",
      "\n",
      "If you need further assistance, please let me know!</s>\n"
     ]
    }
   ],
   "source": [
    "text_output = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "#break on [/INST]\n",
    "text_output = text_output.split(\"[/INST]\")[1]\n",
    "print(text_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepping model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Set up LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,097,152 || all params: 264,507,392 || trainable%: 0.7929\n"
     ]
    }
   ],
   "source": [
    "#LoRA config\n",
    "config = LoraConfig(r=8, lora_alpha=32, target_modules=[\"q_proj\"], lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\")\n",
    "\n",
    "# LoRA trainable version of model\n",
    "model = get_peft_model(model, config)\n",
    "\n",
    "# trainable parameter count\n",
    "model.print_trainable_parameters()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 558/558 [00:00<00:00, 33557.80 examples/s]\n",
      "Map: 100%|██████████| 62/62 [00:00<00:00, 15715.65 examples/s]\n"
     ]
    }
   ],
   "source": [
    "custom_data = load_dataset(\"json\", data_files=\"../../datasets/qa_pairs.json\")\n",
    "\n",
    "split_dataset = custom_data[\"train\"].train_test_split(test_size=0.1)\n",
    "\n",
    "def format_prompt(example):\n",
    "    instruction = example[\"instruction\"]\n",
    "    response = example[\"response\"]\n",
    "    prompt = f\"[INST] {instruction} [/INST]\"\n",
    "    \n",
    "    full_text = f\"{prompt}\\n{response}\"\n",
    "    return {\"example\": full_text}\n",
    "data = split_dataset.map(format_prompt).remove_columns([\"instruction\", \"response\"])\n",
    "#drop instruction and response columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['example'],\n",
      "        num_rows: 558\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['example'],\n",
      "        num_rows: 62\n",
      "    })\n",
      "})\n",
      "{'example': '[INST] How does an LLC differ from a corporation in Washington? [/INST]\\nLLCs offer more flexibility in management and typically simpler taxation3.'}\n",
      "==================================================\n",
      "{'example': '[INST] Can a PLLC have non-professional members? [/INST]\\nGenerally no, all members must be licensed in the relevant profession.'}\n",
      "==================================================\n",
      "{'example': '[INST] Can an LLC have different classes of membership? [/INST]\\nYes, LLCs can  create different classes of membership with varying rights and privileges.'}\n",
      "==================================================\n",
      "{'example': '[INST] Can a Washington LLC operate in other states? [/INST]\\nYes, but you must register as a foreign LLC in each state where you conduct  business.'}\n",
      "==================================================\n",
      "{'example': '[INST] What do I need to do if I want to obtain a federal tax number for my LLC or  corporation?\\u200b [/INST]\\nYou’ll need to obtain a federal ID number (EIN) from the IRS. LLCs and corporations  also need to file specific tax forms, such as Form 1120 for standard corporations or Form 1120S  if electing S-corporation status.'}\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(data)\n",
    "# Print first 5 rows of the training dataset\n",
    "for i in range(5):\n",
    "    print(data[\"train\"][i])  # Prints each row\n",
    "    print(\"=\"*50)  # Separator for readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 558/558 [00:00<00:00, 18884.42 examples/s]\n",
      "Map: 100%|██████████| 62/62 [00:00<00:00, 16734.03 examples/s]\n"
     ]
    }
   ],
   "source": [
    "#create tokenize functoin\n",
    "def tokenize_function(examples):\n",
    "    #extract text\n",
    "    text = examples[\"example\"]\n",
    "    \n",
    "    #tokenize and truncate text\n",
    "    tokenizer.truncation_side = \"left\"\n",
    "    tokenized_inputs = tokenizer(text, return_tensors=\"np\", truncation=True, max_length=512)\n",
    "    \n",
    "    \n",
    "    return tokenized_inputs\n",
    "\n",
    "#tokenize training and validation data\n",
    "tokenized_data = data.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting pad token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "#data collator\n",
    "data_collator = transformers.DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "lr = 2e-4\n",
    "batch_size = 4\n",
    "num_epochs = 10\n",
    "\n",
    "#define training args\n",
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir=\"./output\",\n",
    "    learning_rate=lr,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    weight_decay=0.01,\n",
    "    logging_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=2,\n",
    "    fp16=True,\n",
    "    optim=\"paged_adamw_8bit\"\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='350' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [350/350 27:31, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.824300</td>\n",
       "      <td>1.646378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.417700</td>\n",
       "      <td>1.384386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.207500</td>\n",
       "      <td>1.351078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.080700</td>\n",
       "      <td>1.340305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.975600</td>\n",
       "      <td>1.369766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>1.417781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.801600</td>\n",
       "      <td>1.497388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.734400</td>\n",
       "      <td>1.583008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.683100</td>\n",
       "      <td>1.668267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.648600</td>\n",
       "      <td>1.712029</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#configure trainer\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_data[\"train\"],\n",
    "    eval_dataset=tokenized_data[\"test\"],\n",
    "    args=training_args,\n",
    "    data_collator=data_collator\n",
    "    )\n",
    "\n",
    "#train model\n",
    "model.config.use_cache = False\n",
    "trainer.train()\n",
    "\n",
    "# renable warnings\n",
    "model.config.use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./business_llm/tokenizer_config.json',\n",
       " './business_llm/special_tokens_map.json',\n",
       " './business_llm/tokenizer.model',\n",
       " './business_llm/added_tokens.json',\n",
       " './business_llm/tokenizer.json')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save Model & Tokenizer\n",
    "model.save_pretrained(\"./business_llm\")\n",
    "tokenizer.save_pretrained(\"./business_llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop the model and remove it from the GPU\n",
    "model.cpu()\n",
    "model = None\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at TheBloke/Mistral-7B-Instruct-v0.2-GPTQ were not used when initializing MistralForCausalLM: {'model.layers.17.self_attn.o_proj.bias', 'model.layers.3.mlp.up_proj.bias', 'model.layers.14.mlp.gate_proj.bias', 'model.layers.30.self_attn.k_proj.bias', 'model.layers.11.self_attn.q_proj.bias', 'model.layers.13.mlp.down_proj.bias', 'model.layers.14.self_attn.k_proj.bias', 'model.layers.26.mlp.down_proj.bias', 'model.layers.4.mlp.up_proj.bias', 'model.layers.9.mlp.down_proj.bias', 'model.layers.18.self_attn.v_proj.bias', 'model.layers.31.self_attn.v_proj.bias', 'model.layers.18.mlp.gate_proj.bias', 'model.layers.2.mlp.gate_proj.bias', 'model.layers.21.self_attn.q_proj.bias', 'model.layers.4.self_attn.k_proj.bias', 'model.layers.0.self_attn.o_proj.bias', 'model.layers.5.self_attn.q_proj.bias', 'model.layers.19.mlp.down_proj.bias', 'model.layers.27.mlp.gate_proj.bias', 'model.layers.20.self_attn.k_proj.bias', 'model.layers.28.mlp.down_proj.bias', 'model.layers.9.self_attn.q_proj.bias', 'model.layers.1.self_attn.k_proj.bias', 'model.layers.12.self_attn.k_proj.bias', 'model.layers.21.self_attn.o_proj.bias', 'model.layers.5.mlp.up_proj.bias', 'model.layers.2.self_attn.o_proj.bias', 'model.layers.26.mlp.up_proj.bias', 'model.layers.24.mlp.down_proj.bias', 'model.layers.29.mlp.down_proj.bias', 'model.layers.20.mlp.up_proj.bias', 'model.layers.1.self_attn.q_proj.bias', 'model.layers.20.mlp.down_proj.bias', 'model.layers.30.mlp.up_proj.bias', 'model.layers.10.mlp.down_proj.bias', 'model.layers.3.mlp.down_proj.bias', 'model.layers.7.mlp.down_proj.bias', 'model.layers.5.self_attn.k_proj.bias', 'model.layers.3.self_attn.o_proj.bias', 'model.layers.23.self_attn.q_proj.bias', 'model.layers.7.mlp.up_proj.bias', 'model.layers.16.mlp.gate_proj.bias', 'model.layers.27.self_attn.o_proj.bias', 'model.layers.30.mlp.gate_proj.bias', 'model.layers.29.mlp.up_proj.bias', 'model.layers.12.self_attn.q_proj.bias', 'model.layers.11.mlp.down_proj.bias', 'model.layers.8.self_attn.q_proj.bias', 'model.layers.2.self_attn.v_proj.bias', 'model.layers.15.self_attn.v_proj.bias', 'model.layers.21.self_attn.v_proj.bias', 'model.layers.24.self_attn.o_proj.bias', 'model.layers.13.self_attn.v_proj.bias', 'model.layers.13.mlp.gate_proj.bias', 'model.layers.26.self_attn.v_proj.bias', 'model.layers.28.self_attn.q_proj.bias', 'model.layers.15.mlp.up_proj.bias', 'model.layers.9.self_attn.k_proj.bias', 'model.layers.19.self_attn.o_proj.bias', 'model.layers.23.self_attn.o_proj.bias', 'model.layers.27.self_attn.k_proj.bias', 'model.layers.22.self_attn.v_proj.bias', 'model.layers.10.mlp.up_proj.bias', 'model.layers.22.mlp.down_proj.bias', 'model.layers.24.mlp.gate_proj.bias', 'model.layers.31.self_attn.k_proj.bias', 'model.layers.0.self_attn.q_proj.bias', 'model.layers.9.self_attn.o_proj.bias', 'model.layers.5.self_attn.o_proj.bias', 'model.layers.12.mlp.down_proj.bias', 'model.layers.16.self_attn.v_proj.bias', 'model.layers.2.self_attn.k_proj.bias', 'model.layers.23.self_attn.k_proj.bias', 'model.layers.15.mlp.gate_proj.bias', 'model.layers.3.self_attn.v_proj.bias', 'model.layers.11.self_attn.o_proj.bias', 'model.layers.6.self_attn.o_proj.bias', 'model.layers.26.self_attn.o_proj.bias', 'model.layers.7.self_attn.k_proj.bias', 'model.layers.18.mlp.down_proj.bias', 'model.layers.19.self_attn.v_proj.bias', 'model.layers.26.mlp.gate_proj.bias', 'model.layers.18.self_attn.k_proj.bias', 'model.layers.13.self_attn.q_proj.bias', 'model.layers.25.self_attn.o_proj.bias', 'model.layers.20.mlp.gate_proj.bias', 'model.layers.11.self_attn.k_proj.bias', 'model.layers.8.mlp.down_proj.bias', 'model.layers.1.self_attn.v_proj.bias', 'model.layers.4.self_attn.q_proj.bias', 'model.layers.4.self_attn.o_proj.bias', 'model.layers.21.mlp.up_proj.bias', 'model.layers.11.mlp.up_proj.bias', 'model.layers.20.self_attn.o_proj.bias', 'model.layers.22.mlp.up_proj.bias', 'model.layers.23.mlp.up_proj.bias', 'model.layers.10.self_attn.v_proj.bias', 'model.layers.20.self_attn.v_proj.bias', 'model.layers.27.self_attn.v_proj.bias', 'model.layers.29.self_attn.v_proj.bias', 'model.layers.9.self_attn.v_proj.bias', 'model.layers.18.mlp.up_proj.bias', 'model.layers.31.mlp.gate_proj.bias', 'model.layers.31.mlp.down_proj.bias', 'model.layers.22.mlp.gate_proj.bias', 'model.layers.22.self_attn.k_proj.bias', 'model.layers.14.mlp.up_proj.bias', 'model.layers.14.mlp.down_proj.bias', 'model.layers.4.mlp.down_proj.bias', 'model.layers.6.self_attn.q_proj.bias', 'model.layers.6.self_attn.v_proj.bias', 'model.layers.9.mlp.gate_proj.bias', 'model.layers.28.mlp.up_proj.bias', 'model.layers.22.self_attn.q_proj.bias', 'model.layers.11.self_attn.v_proj.bias', 'model.layers.18.self_attn.o_proj.bias', 'model.layers.7.self_attn.v_proj.bias', 'model.layers.16.self_attn.o_proj.bias', 'model.layers.27.mlp.up_proj.bias', 'model.layers.13.self_attn.k_proj.bias', 'model.layers.24.mlp.up_proj.bias', 'model.layers.25.mlp.gate_proj.bias', 'model.layers.9.mlp.up_proj.bias', 'model.layers.17.self_attn.v_proj.bias', 'model.layers.15.self_attn.k_proj.bias', 'model.layers.8.self_attn.k_proj.bias', 'model.layers.19.self_attn.k_proj.bias', 'model.layers.3.self_attn.k_proj.bias', 'model.layers.1.mlp.down_proj.bias', 'model.layers.5.mlp.gate_proj.bias', 'model.layers.6.mlp.gate_proj.bias', 'model.layers.0.mlp.down_proj.bias', 'model.layers.15.mlp.down_proj.bias', 'model.layers.0.self_attn.v_proj.bias', 'model.layers.4.mlp.gate_proj.bias', 'model.layers.30.self_attn.o_proj.bias', 'model.layers.19.mlp.gate_proj.bias', 'model.layers.23.mlp.gate_proj.bias', 'model.layers.27.self_attn.q_proj.bias', 'model.layers.17.self_attn.q_proj.bias', 'model.layers.15.self_attn.o_proj.bias', 'model.layers.24.self_attn.q_proj.bias', 'model.layers.12.mlp.up_proj.bias', 'model.layers.10.self_attn.o_proj.bias', 'model.layers.28.self_attn.o_proj.bias', 'model.layers.3.self_attn.q_proj.bias', 'model.layers.0.self_attn.k_proj.bias', 'model.layers.7.mlp.gate_proj.bias', 'model.layers.23.mlp.down_proj.bias', 'model.layers.8.mlp.gate_proj.bias', 'model.layers.0.mlp.up_proj.bias', 'model.layers.31.mlp.up_proj.bias', 'model.layers.7.self_attn.o_proj.bias', 'model.layers.14.self_attn.o_proj.bias', 'model.layers.21.mlp.down_proj.bias', 'model.layers.1.mlp.gate_proj.bias', 'model.layers.2.mlp.down_proj.bias', 'model.layers.25.mlp.down_proj.bias', 'model.layers.10.self_attn.q_proj.bias', 'model.layers.25.mlp.up_proj.bias', 'model.layers.19.self_attn.q_proj.bias', 'model.layers.27.mlp.down_proj.bias', 'model.layers.21.self_attn.k_proj.bias', 'model.layers.7.self_attn.q_proj.bias', 'model.layers.29.self_attn.q_proj.bias', 'model.layers.10.mlp.gate_proj.bias', 'model.layers.3.mlp.gate_proj.bias', 'model.layers.22.self_attn.o_proj.bias', 'model.layers.29.mlp.gate_proj.bias', 'model.layers.12.self_attn.v_proj.bias', 'model.layers.12.self_attn.o_proj.bias', 'model.layers.17.mlp.gate_proj.bias', 'model.layers.17.mlp.down_proj.bias', 'model.layers.2.self_attn.q_proj.bias', 'model.layers.5.mlp.down_proj.bias', 'model.layers.6.mlp.up_proj.bias', 'model.layers.13.mlp.up_proj.bias', 'model.layers.28.mlp.gate_proj.bias', 'model.layers.30.self_attn.q_proj.bias', 'model.layers.6.mlp.down_proj.bias', 'model.layers.4.self_attn.v_proj.bias', 'model.layers.19.mlp.up_proj.bias', 'model.layers.5.self_attn.v_proj.bias', 'model.layers.16.mlp.down_proj.bias', 'model.layers.30.self_attn.v_proj.bias', 'model.layers.24.self_attn.v_proj.bias', 'model.layers.6.self_attn.k_proj.bias', 'model.layers.2.mlp.up_proj.bias', 'model.layers.28.self_attn.k_proj.bias', 'model.layers.21.mlp.gate_proj.bias', 'model.layers.25.self_attn.q_proj.bias', 'model.layers.23.self_attn.v_proj.bias', 'model.layers.11.mlp.gate_proj.bias', 'model.layers.8.self_attn.v_proj.bias', 'model.layers.16.mlp.up_proj.bias', 'model.layers.25.self_attn.v_proj.bias', 'model.layers.12.mlp.gate_proj.bias', 'model.layers.29.self_attn.k_proj.bias', 'model.layers.26.self_attn.k_proj.bias', 'model.layers.10.self_attn.k_proj.bias', 'model.layers.18.self_attn.q_proj.bias', 'model.layers.31.self_attn.o_proj.bias', 'model.layers.8.mlp.up_proj.bias', 'model.layers.14.self_attn.v_proj.bias', 'model.layers.25.self_attn.k_proj.bias', 'model.layers.8.self_attn.o_proj.bias', 'model.layers.17.mlp.up_proj.bias', 'model.layers.30.mlp.down_proj.bias', 'model.layers.29.self_attn.o_proj.bias', 'model.layers.0.mlp.gate_proj.bias', 'model.layers.31.self_attn.q_proj.bias', 'model.layers.26.self_attn.q_proj.bias', 'model.layers.28.self_attn.v_proj.bias', 'model.layers.1.self_attn.o_proj.bias', 'model.layers.20.self_attn.q_proj.bias', 'model.layers.14.self_attn.q_proj.bias', 'model.layers.13.self_attn.o_proj.bias', 'model.layers.15.self_attn.q_proj.bias', 'model.layers.16.self_attn.q_proj.bias', 'model.layers.16.self_attn.k_proj.bias', 'model.layers.24.self_attn.k_proj.bias', 'model.layers.17.self_attn.k_proj.bias', 'model.layers.1.mlp.up_proj.bias'}\n",
      "- This IS expected if you are initializing MistralForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MistralForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "To start a non-profit in Washington, you need to file a Non-Profit Certificate of Incorporation, create bylaws, and apply for 501(c)(3) status. This will allow you to receive tax-deductible donations and apply for grants. You'll also need to hold regular meetings and maintain records. If you plan to hire employees, you may need to register for an EIN and pay payroll taxes.\n",
      "\n",
      "If you have any questions or need assistance, feel free to ask.\n",
      "\n",
      "As a non-profit, you will be able to receive tax-deductible donations and apply for grants, but you will need to maintain records and hold\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load Base Model\n",
    "base_model_name = \"TheBloke/Mistral-7B-Instruct-v0.2-GPTQ\"\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_name, device_map=\"auto\")\n",
    "\n",
    "# Load Fine-Tuned Adapter\n",
    "model = PeftModel.from_pretrained(base_model, \"./business_llm\")\n",
    "\n",
    "# Load Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./business_llm\", use_fast=True)\n",
    "\n",
    "# Generate Business Type Recommendation\n",
    "def generate_response(user_input):\n",
    "    prompt = f\"[INST] {instructions_string}\\n{user_input} [/INST]\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    output = model.generate(**inputs, max_new_tokens=150)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Test Your Model\n",
    "comment = \"I want to start a foodbank in Washington, how do I?\"\n",
    "#user_query = \"I’m planning to start a small bakery where I sell homemade pastries. I want to protect my personal assets in case something goes wrong, but I also want to keep my taxes simple. I don’t expect to have any employees right away, but I might hire one or two in the future. What business structure would be best for me?\"\n",
    "response = generate_response(comment)\n",
    "#break on [/INST]\n",
    "text_output = response.split(\"[/INST]\")[1]\n",
    "print(text_output)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LegalEaseWSL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
