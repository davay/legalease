{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bphilipose/miniconda3/envs/LegalEaseWSL/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from peft import prepare_model_for_kbit_training\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.empty_cache()\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bphilipose/miniconda3/envs/LegalEaseWSL/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\n",
      "/home/bphilipose/miniconda3/envs/LegalEaseWSL/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n",
      "/home/bphilipose/miniconda3/envs/LegalEaseWSL/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float16)\n",
      "CUDA extension not installed.\n",
      "CUDA extension not installed.\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
      "Some weights of the model checkpoint at TheBloke/Mistral-7B-Instruct-v0.2-GPTQ were not used when initializing MistralForCausalLM: {'model.layers.22.self_attn.v_proj.bias', 'model.layers.27.mlp.gate_proj.bias', 'model.layers.31.self_attn.o_proj.bias', 'model.layers.29.self_attn.v_proj.bias', 'model.layers.15.self_attn.v_proj.bias', 'model.layers.6.mlp.gate_proj.bias', 'model.layers.25.self_attn.q_proj.bias', 'model.layers.26.self_attn.v_proj.bias', 'model.layers.20.self_attn.v_proj.bias', 'model.layers.25.mlp.down_proj.bias', 'model.layers.24.mlp.up_proj.bias', 'model.layers.21.mlp.up_proj.bias', 'model.layers.5.self_attn.o_proj.bias', 'model.layers.20.mlp.down_proj.bias', 'model.layers.14.mlp.gate_proj.bias', 'model.layers.28.mlp.gate_proj.bias', 'model.layers.19.mlp.up_proj.bias', 'model.layers.29.self_attn.o_proj.bias', 'model.layers.27.self_attn.v_proj.bias', 'model.layers.23.mlp.down_proj.bias', 'model.layers.30.self_attn.o_proj.bias', 'model.layers.3.mlp.up_proj.bias', 'model.layers.29.mlp.up_proj.bias', 'model.layers.18.self_attn.o_proj.bias', 'model.layers.24.mlp.gate_proj.bias', 'model.layers.0.self_attn.v_proj.bias', 'model.layers.20.mlp.up_proj.bias', 'model.layers.13.self_attn.q_proj.bias', 'model.layers.19.mlp.down_proj.bias', 'model.layers.18.mlp.down_proj.bias', 'model.layers.22.mlp.up_proj.bias', 'model.layers.27.self_attn.o_proj.bias', 'model.layers.5.self_attn.q_proj.bias', 'model.layers.28.self_attn.q_proj.bias', 'model.layers.31.self_attn.q_proj.bias', 'model.layers.17.mlp.gate_proj.bias', 'model.layers.21.self_attn.k_proj.bias', 'model.layers.17.self_attn.o_proj.bias', 'model.layers.26.mlp.down_proj.bias', 'model.layers.5.self_attn.k_proj.bias', 'model.layers.8.self_attn.k_proj.bias', 'model.layers.29.self_attn.q_proj.bias', 'model.layers.18.self_attn.k_proj.bias', 'model.layers.4.self_attn.k_proj.bias', 'model.layers.24.mlp.down_proj.bias', 'model.layers.24.self_attn.v_proj.bias', 'model.layers.0.self_attn.k_proj.bias', 'model.layers.16.mlp.up_proj.bias', 'model.layers.30.mlp.down_proj.bias', 'model.layers.8.self_attn.o_proj.bias', 'model.layers.21.self_attn.o_proj.bias', 'model.layers.18.self_attn.v_proj.bias', 'model.layers.21.self_attn.q_proj.bias', 'model.layers.10.mlp.gate_proj.bias', 'model.layers.18.self_attn.q_proj.bias', 'model.layers.16.self_attn.v_proj.bias', 'model.layers.11.mlp.gate_proj.bias', 'model.layers.16.self_attn.q_proj.bias', 'model.layers.17.self_attn.v_proj.bias', 'model.layers.17.mlp.down_proj.bias', 'model.layers.16.self_attn.k_proj.bias', 'model.layers.11.self_attn.k_proj.bias', 'model.layers.28.self_attn.k_proj.bias', 'model.layers.9.self_attn.q_proj.bias', 'model.layers.26.mlp.gate_proj.bias', 'model.layers.10.mlp.up_proj.bias', 'model.layers.30.self_attn.q_proj.bias', 'model.layers.25.mlp.gate_proj.bias', 'model.layers.27.self_attn.k_proj.bias', 'model.layers.10.self_attn.k_proj.bias', 'model.layers.22.self_attn.o_proj.bias', 'model.layers.26.self_attn.o_proj.bias', 'model.layers.22.self_attn.k_proj.bias', 'model.layers.27.mlp.up_proj.bias', 'model.layers.0.mlp.up_proj.bias', 'model.layers.16.mlp.gate_proj.bias', 'model.layers.3.self_attn.v_proj.bias', 'model.layers.8.self_attn.q_proj.bias', 'model.layers.8.mlp.down_proj.bias', 'model.layers.28.self_attn.v_proj.bias', 'model.layers.9.self_attn.k_proj.bias', 'model.layers.9.mlp.down_proj.bias', 'model.layers.15.self_attn.k_proj.bias', 'model.layers.17.self_attn.q_proj.bias', 'model.layers.7.self_attn.q_proj.bias', 'model.layers.14.self_attn.q_proj.bias', 'model.layers.0.self_attn.o_proj.bias', 'model.layers.4.self_attn.q_proj.bias', 'model.layers.23.mlp.gate_proj.bias', 'model.layers.19.self_attn.k_proj.bias', 'model.layers.12.self_attn.k_proj.bias', 'model.layers.19.self_attn.q_proj.bias', 'model.layers.9.mlp.up_proj.bias', 'model.layers.18.mlp.gate_proj.bias', 'model.layers.15.mlp.gate_proj.bias', 'model.layers.24.self_attn.o_proj.bias', 'model.layers.15.self_attn.q_proj.bias', 'model.layers.2.self_attn.v_proj.bias', 'model.layers.24.self_attn.q_proj.bias', 'model.layers.3.mlp.down_proj.bias', 'model.layers.31.mlp.up_proj.bias', 'model.layers.15.mlp.up_proj.bias', 'model.layers.30.self_attn.v_proj.bias', 'model.layers.23.mlp.up_proj.bias', 'model.layers.5.mlp.up_proj.bias', 'model.layers.26.self_attn.k_proj.bias', 'model.layers.1.mlp.gate_proj.bias', 'model.layers.0.mlp.gate_proj.bias', 'model.layers.21.mlp.gate_proj.bias', 'model.layers.23.self_attn.k_proj.bias', 'model.layers.11.self_attn.v_proj.bias', 'model.layers.14.mlp.up_proj.bias', 'model.layers.19.self_attn.v_proj.bias', 'model.layers.28.mlp.up_proj.bias', 'model.layers.29.mlp.down_proj.bias', 'model.layers.3.self_attn.o_proj.bias', 'model.layers.9.mlp.gate_proj.bias', 'model.layers.19.self_attn.o_proj.bias', 'model.layers.30.self_attn.k_proj.bias', 'model.layers.31.self_attn.k_proj.bias', 'model.layers.7.self_attn.o_proj.bias', 'model.layers.8.mlp.up_proj.bias', 'model.layers.10.mlp.down_proj.bias', 'model.layers.10.self_attn.q_proj.bias', 'model.layers.5.mlp.gate_proj.bias', 'model.layers.9.self_attn.v_proj.bias', 'model.layers.3.self_attn.q_proj.bias', 'model.layers.25.self_attn.o_proj.bias', 'model.layers.23.self_attn.o_proj.bias', 'model.layers.4.self_attn.v_proj.bias', 'model.layers.6.self_attn.k_proj.bias', 'model.layers.13.self_attn.o_proj.bias', 'model.layers.20.self_attn.k_proj.bias', 'model.layers.1.self_attn.q_proj.bias', 'model.layers.11.mlp.up_proj.bias', 'model.layers.23.self_attn.v_proj.bias', 'model.layers.31.mlp.gate_proj.bias', 'model.layers.15.self_attn.o_proj.bias', 'model.layers.14.mlp.down_proj.bias', 'model.layers.23.self_attn.q_proj.bias', 'model.layers.0.self_attn.q_proj.bias', 'model.layers.1.mlp.down_proj.bias', 'model.layers.17.self_attn.k_proj.bias', 'model.layers.12.mlp.up_proj.bias', 'model.layers.14.self_attn.k_proj.bias', 'model.layers.2.self_attn.k_proj.bias', 'model.layers.30.mlp.up_proj.bias', 'model.layers.14.self_attn.v_proj.bias', 'model.layers.4.self_attn.o_proj.bias', 'model.layers.11.mlp.down_proj.bias', 'model.layers.7.self_attn.v_proj.bias', 'model.layers.19.mlp.gate_proj.bias', 'model.layers.20.self_attn.q_proj.bias', 'model.layers.8.mlp.gate_proj.bias', 'model.layers.18.mlp.up_proj.bias', 'model.layers.27.mlp.down_proj.bias', 'model.layers.27.self_attn.q_proj.bias', 'model.layers.13.mlp.up_proj.bias', 'model.layers.6.self_attn.o_proj.bias', 'model.layers.2.self_attn.q_proj.bias', 'model.layers.12.self_attn.o_proj.bias', 'model.layers.26.self_attn.q_proj.bias', 'model.layers.21.self_attn.v_proj.bias', 'model.layers.28.mlp.down_proj.bias', 'model.layers.16.self_attn.o_proj.bias', 'model.layers.22.self_attn.q_proj.bias', 'model.layers.21.mlp.down_proj.bias', 'model.layers.2.mlp.up_proj.bias', 'model.layers.1.self_attn.v_proj.bias', 'model.layers.12.self_attn.v_proj.bias', 'model.layers.11.self_attn.q_proj.bias', 'model.layers.26.mlp.up_proj.bias', 'model.layers.2.self_attn.o_proj.bias', 'model.layers.5.mlp.down_proj.bias', 'model.layers.7.mlp.down_proj.bias', 'model.layers.25.self_attn.v_proj.bias', 'model.layers.1.self_attn.o_proj.bias', 'model.layers.12.self_attn.q_proj.bias', 'model.layers.2.mlp.down_proj.bias', 'model.layers.5.self_attn.v_proj.bias', 'model.layers.7.mlp.up_proj.bias', 'model.layers.28.self_attn.o_proj.bias', 'model.layers.25.mlp.up_proj.bias', 'model.layers.10.self_attn.o_proj.bias', 'model.layers.4.mlp.up_proj.bias', 'model.layers.6.self_attn.q_proj.bias', 'model.layers.22.mlp.gate_proj.bias', 'model.layers.20.mlp.gate_proj.bias', 'model.layers.6.mlp.up_proj.bias', 'model.layers.30.mlp.gate_proj.bias', 'model.layers.1.mlp.up_proj.bias', 'model.layers.2.mlp.gate_proj.bias', 'model.layers.24.self_attn.k_proj.bias', 'model.layers.16.mlp.down_proj.bias', 'model.layers.7.self_attn.k_proj.bias', 'model.layers.3.mlp.gate_proj.bias', 'model.layers.4.mlp.down_proj.bias', 'model.layers.10.self_attn.v_proj.bias', 'model.layers.0.mlp.down_proj.bias', 'model.layers.11.self_attn.o_proj.bias', 'model.layers.13.self_attn.k_proj.bias', 'model.layers.14.self_attn.o_proj.bias', 'model.layers.8.self_attn.v_proj.bias', 'model.layers.31.self_attn.v_proj.bias', 'model.layers.17.mlp.up_proj.bias', 'model.layers.4.mlp.gate_proj.bias', 'model.layers.6.self_attn.v_proj.bias', 'model.layers.22.mlp.down_proj.bias', 'model.layers.20.self_attn.o_proj.bias', 'model.layers.13.mlp.gate_proj.bias', 'model.layers.6.mlp.down_proj.bias', 'model.layers.7.mlp.gate_proj.bias', 'model.layers.15.mlp.down_proj.bias', 'model.layers.9.self_attn.o_proj.bias', 'model.layers.3.self_attn.k_proj.bias', 'model.layers.25.self_attn.k_proj.bias', 'model.layers.29.self_attn.k_proj.bias', 'model.layers.29.mlp.gate_proj.bias', 'model.layers.1.self_attn.k_proj.bias', 'model.layers.12.mlp.down_proj.bias', 'model.layers.13.mlp.down_proj.bias', 'model.layers.31.mlp.down_proj.bias', 'model.layers.13.self_attn.v_proj.bias', 'model.layers.12.mlp.gate_proj.bias'}\n",
      "- This IS expected if you are initializing MistralForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MistralForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = \"TheBloke/Mistral-7B-Instruct-v0.2-GPTQ\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map={\"\": 0}, trust_remote_code=False, revision=\"main\")\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Apply tokenization\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment = \"I want to start a foodbank in Washington, how do I?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt with instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions_string = f\"\"\"You are an expert business consultant designed to help users determine the most suitable business structure for their needs. You will guide the user toward choosing one of the following options:\n",
    "- **LLC (Limited Liability Company)**\n",
    "- **Non-Profit Organization**\n",
    "- **S-Corporation**\n",
    "\n",
    "Do not recommend General Partnerships, Sole Proprietorships, or C-Corporations.\n",
    "\n",
    "### **Your Approach**\n",
    "1. **Ask Clarifying Questions:** If the user is unsure, gather details about their business, such as liability concerns, taxation preferences, funding sources, and purpose.\n",
    "2. **Analyze Needs:** Based on their responses, determine whether an **LLC, Non-Profit, or S-Corp** is the best fit.\n",
    "3. **Provide a Recommendation:** Clearly explain **why** the suggested structure is the best choice.\n",
    "4. **Address Concerns:** If the user expresses doubts, clarify any misconceptions and guide them toward the best decision.\n",
    "\n",
    "### **Response Format**\n",
    "- **Direct Answer:** If the user describes their business clearly, provide a structured recommendation.\n",
    "- **Guided Questions:** If the user is unsure, ask simple questions to help them decide.\n",
    "- **Comparison (if needed):** If two structures could work, briefly compare them.\n",
    "\n",
    "### **Example Scenarios**\n",
    "#### **User Input 1:**  \n",
    "*\"I’m starting a consulting business with a partner. We want liability protection and pass-through taxation.\"*  \n",
    "**LLM Response:**  \n",
    "*\"An LLC would be ideal for you. It offers **liability protection**, meaning your personal assets are safe, and it allows **pass-through taxation**, so profits aren’t taxed twice. You’ll also have flexibility in management and ownership. Would you like help with the next steps?\"*\n",
    "\n",
    "#### **User Input 2:**  \n",
    "*\"I want to create an organization to help underprivileged kids get scholarships.\"*  \n",
    "**LLM Response:**  \n",
    "*\"A Non-Profit is the best choice for your mission. It allows you to receive tax-deductible donations, apply for grants, and focus on community impact. To qualify, your organization must serve the public interest. Do you need help with 501(c)(3) registration?\"*\n",
    "\n",
    "#### **User Input 3:**  \n",
    "*\"I run a small business and want liability protection, but I also want to save on self-employment taxes.\"*  \n",
    "**LLM Response:**  \n",
    "*\"An S-Corporation might be your best option. It provides **liability protection**, and unlike an LLC, it allows you to pay yourself a salary while reducing self-employment taxes. Would you like a quick breakdown of how S-Corp taxation works?\"*\n",
    "\n",
    "**Now, respond to the following user inquiry:**\n",
    "\"\"\"\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "     \n",
    "#prompt=f'''[INST] {comment} [/INST]'''\n",
    "\n",
    "prompt_template = lambda in_comment: f'''[INST] {instructions_string} \\n{in_comment} \\n[/INST]'''\n",
    "prompt = prompt_template(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "#generate output\n",
    "outputs = model.generate(input_ids=inputs[\"input_ids\"].to('cuda'), max_new_tokens=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] You are an expert business consultant designed to help users determine the most suitable business structure for their needs. You will guide the user toward choosing one of the following options:\n",
      "- **LLC (Limited Liability Company)**\n",
      "- **Non-Profit Organization**\n",
      "- **S-Corporation**\n",
      "\n",
      "Do not recommend General Partnerships, Sole Proprietorships, or C-Corporations.\n",
      "\n",
      "### **Your Approach**\n",
      "1. **Ask Clarifying Questions:** If the user is unsure, gather details about their business, such as liability concerns, taxation preferences, funding sources, and purpose.\n",
      "2. **Analyze Needs:** Based on their responses, determine whether an **LLC, Non-Profit, or S-Corp** is the best fit.\n",
      "3. **Provide a Recommendation:** Clearly explain **why** the suggested structure is the best choice.\n",
      "4. **Address Concerns:** If the user expresses doubts, clarify any misconceptions and guide them toward the best decision.\n",
      "\n",
      "### **Response Format**\n",
      "- **Direct Answer:** If the user describes their business clearly, provide a structured recommendation.\n",
      "- **Guided Questions:** If the user is unsure, ask simple questions to help them decide.\n",
      "- **Comparison (if needed):** If two structures could work, briefly compare them.\n",
      "\n",
      "### **Example Scenarios**\n",
      "#### **User Input 1:**  \n",
      "*\"I’m starting a consulting business with a partner. We want liability protection and pass-through taxation.\"*  \n",
      "**LLM Response:**  \n",
      "*\"An LLC would be ideal for you. It offers **liability protection**, meaning your personal assets are safe, and it allows **pass-through taxation**, so profits aren’t taxed twice. You’ll also have flexibility in management and ownership. Would you like help with the next steps?\"*\n",
      "\n",
      "#### **User Input 2:**  \n",
      "*\"I want to create an organization to help underprivileged kids get scholarships.\"*  \n",
      "**LLM Response:**  \n",
      "*\"A Non-Profit is the best choice for your mission. It allows you to receive tax-deductible donations, apply for grants, and focus on community impact. To qualify, your organization must serve the public interest. Do you need help with 501(c)(3) registration?\"*\n",
      "\n",
      "#### **User Input 3:**  \n",
      "*\"I run a small business and want liability protection, but I also want to save on self-employment taxes.\"*  \n",
      "**LLM Response:**  \n",
      "*\"An S-Corporation might be your best option. It provides **liability protection**, and unlike an LLC, it allows you to pay yourself a salary while reducing self-employment taxes. Would you like a quick breakdown of how S-Corp taxation works?\"*\n",
      "\n",
      "**Now, respond to the following user inquiry:**\n",
      " \n",
      "I want to start a foodbank in Washington, how do I? \n",
      "[/INST] **LLM Response:**\n",
      "\n",
      "Starting a food bank in Washington involves several steps to establish a non-profit organization. Here's a simplified guide:\n",
      "\n",
      "1. **Choose a Name:** Select a unique name for your food bank that is not already in use.\n",
      "2. **Create a Board of Directors:** Assemble a board of directors to oversee the organization's operations.\n",
      "3. **File Articles of Incorporation:** File articles of incorporation with the Washington Secretary of State to form a non-profit corporation.\n",
      "4. **Apply for Tax-Exempt Status:** Apply for tax-exempt status from the Internal Revenue Service (IRS) as a 501(c)(3) organization.\n",
      "5. **Register with the Washington State Department of Revenue:** Register with the Washington State Department of Revenue to collect and report sales tax.\n",
      "6. **Obtain Necessary Permits and Licenses:** Obtain any necessary permits and licenses, such as a food service permit, from local and state agencies.\n",
      "7. **Set Up a Bank Account:** Set up a bank account for your non-profit organization.\n",
      "8. **Create Bylaws and Policies:** Develop bylaws, policies, and procedures for your food bank.\n",
      "9. **Raise Funds and Collect Donations:** Begin raising funds and collecting donations to support your food bank's mission.\n",
      "\n",
      "If you need further assistance, please let me know!</s>\n"
     ]
    }
   ],
   "source": [
    "text_output = tokenizer.batch_decode(outputs)[0]\n",
    "print(text_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepping model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Set up LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,097,152 || all params: 264,507,392 || trainable%: 0.7929\n"
     ]
    }
   ],
   "source": [
    "#LoRA config\n",
    "config = LoraConfig(r=8, lora_alpha=32, target_modules=[\"q_proj\"], lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\")\n",
    "\n",
    "# LoRA trainable version of model\n",
    "model = get_peft_model(model, config)\n",
    "\n",
    "# trainable parameter count\n",
    "model.print_trainable_parameters()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset\n",
    "data = load_dataset(\"shawhin/shawgpt-youtube-comments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 92/92 [00:00<00:00, 17784.76 examples/s]\n",
      "Map: 100%|██████████| 11/11 [00:00<00:00, 4545.10 examples/s]\n"
     ]
    }
   ],
   "source": [
    "custom_data = load_dataset(\"json\", data_files=\"business_qlora.jsonl\")\n",
    "split_dataset = custom_data[\"train\"].train_test_split(test_size=0.1)\n",
    "\n",
    "def format_prompt(example):\n",
    "    instruction = example[\"instruction\"]\n",
    "    response = example[\"response\"]\n",
    "    prompt = f\"[INST] {instruction} [/INST]\"\n",
    "    \n",
    "    full_text = f\"{prompt}\\n{response}\"\n",
    "    return {\"example\": full_text}\n",
    "data = split_dataset.map(format_prompt).remove_columns([\"instruction\", \"response\"])\n",
    "#drop instruction and response columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['example'],\n",
      "        num_rows: 92\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['example'],\n",
      "        num_rows: 11\n",
      "    })\n",
      "})\n",
      "{'example': '[INST] I plan to open a marketing consulting firm with a few business partners and want to avoid double taxation. What is the best structure? [/INST]\\nAn S-Corporation (S-Corp) would be ideal. It allows you to pass business income, losses, deductions, and credits through to shareholders, avoiding corporate taxation. This ensures that income is only taxed once at the shareholder level.'}\n",
      "==================================================\n",
      "{'example': \"[INST] I want to form a charity to raise funds for children's education. What structure should I choose? [/INST]\\nA Non-Profit Corporation is the appropriate choice. You must register under state law and apply for IRS 501(c)(3) tax-exempt status. This will allow you to accept tax-deductible donations and avoid paying federal income tax on earnings.\"}\n",
      "==================================================\n",
      "{'example': '[INST] I am opening a bakery and want liability protection. Should I choose an LLC or an S-Corp? [/INST]\\nAn LLC is typically the best choice for a small bakery, as it provides liability protection and simpler management. If your revenue is high, you can elect S-Corp taxation for tax benefits.'}\n",
      "==================================================\n",
      "{'example': '[INST] Can an LLC be taxed as an S-Corporation? [/INST]\\nYes, an LLC can elect to be taxed as an S-Corp by filing IRS Form 2553. This can help business owners reduce self-employment taxes by splitting income between salary and distributions.'}\n",
      "==================================================\n",
      "{'example': '[INST] I want to start a nonprofit organization focused on community outreach. What legal requirements should I follow? [/INST]\\nTo start a nonprofit, you must register as a Non-Profit Corporation, file Articles of Incorporation, apply for IRS 501(c)(3) tax-exempt status, and establish bylaws and a board of directors.'}\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(data)\n",
    "# Print first 5 rows of the training dataset\n",
    "for i in range(5):\n",
    "    print(data[\"train\"][i])  # Prints each row\n",
    "    print(\"=\"*50)  # Separator for readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 92/92 [00:00<00:00, 11496.04 examples/s]\n",
      "Map: 100%|██████████| 11/11 [00:00<00:00, 3947.07 examples/s]\n"
     ]
    }
   ],
   "source": [
    "#create tokenize functoin\n",
    "def tokenize_function(examples):\n",
    "    #extract text\n",
    "    text = examples[\"example\"]\n",
    "    \n",
    "    #tokenize and truncate text\n",
    "    tokenizer.truncation_side = \"left\"\n",
    "    tokenized_inputs = tokenizer(text, return_tensors=\"np\", truncation=True, max_length=512)\n",
    "    \n",
    "    \n",
    "    return tokenized_inputs\n",
    "\n",
    "#tokenize training and validation data\n",
    "tokenized_data = data.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting pad token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "#data collator\n",
    "data_collator = transformers.DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bphilipose/miniconda3/envs/LegalEaseWSL/lib/python3.11/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#hyperparameters\n",
    "lr = 2e-4\n",
    "batch_size = 4\n",
    "num_epochs = 10\n",
    "\n",
    "#define training args\n",
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir=\"./output\",\n",
    "    learning_rate=lr,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    weight_decay=0.01,\n",
    "    logging_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=2,\n",
    "    fp16=True,\n",
    "    optim=\"paged_adamw_8bit\"\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bphilipose/miniconda3/envs/LegalEaseWSL/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 03:48, Epoch 8/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.914600</td>\n",
       "      <td>3.125748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.632700</td>\n",
       "      <td>2.212230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.920200</td>\n",
       "      <td>1.689648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.477900</td>\n",
       "      <td>1.390056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.247900</td>\n",
       "      <td>1.249396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.104800</td>\n",
       "      <td>1.156360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.983400</td>\n",
       "      <td>1.088584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.910300</td>\n",
       "      <td>1.056372</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bphilipose/miniconda3/envs/LegalEaseWSL/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/bphilipose/miniconda3/envs/LegalEaseWSL/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/bphilipose/miniconda3/envs/LegalEaseWSL/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/bphilipose/miniconda3/envs/LegalEaseWSL/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/bphilipose/miniconda3/envs/LegalEaseWSL/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/bphilipose/miniconda3/envs/LegalEaseWSL/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/bphilipose/miniconda3/envs/LegalEaseWSL/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/bphilipose/miniconda3/envs/LegalEaseWSL/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#configure trainer\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_data[\"train\"],\n",
    "    eval_dataset=tokenized_data[\"test\"],\n",
    "    args=training_args,\n",
    "    data_collator=data_collator\n",
    "    )\n",
    "\n",
    "#train model\n",
    "model.config.use_cache = False\n",
    "trainer.train()\n",
    "\n",
    "# renable warnings\n",
    "model.config.use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./business_llm/tokenizer_config.json',\n",
       " './business_llm/special_tokens_map.json',\n",
       " './business_llm/tokenizer.model',\n",
       " './business_llm/added_tokens.json',\n",
       " './business_llm/tokenizer.json')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save Model & Tokenizer\n",
    "model.save_pretrained(\"./business_llm\")\n",
    "tokenizer.save_pretrained(\"./business_llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop the model and remove it from the GPU\n",
    "model.cpu()\n",
    "model = None\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at TheBloke/Mistral-7B-Instruct-v0.2-GPTQ were not used when initializing MistralForCausalLM: {'model.layers.22.self_attn.v_proj.bias', 'model.layers.27.mlp.gate_proj.bias', 'model.layers.31.self_attn.o_proj.bias', 'model.layers.29.self_attn.v_proj.bias', 'model.layers.15.self_attn.v_proj.bias', 'model.layers.6.mlp.gate_proj.bias', 'model.layers.25.self_attn.q_proj.bias', 'model.layers.26.self_attn.v_proj.bias', 'model.layers.20.self_attn.v_proj.bias', 'model.layers.25.mlp.down_proj.bias', 'model.layers.24.mlp.up_proj.bias', 'model.layers.21.mlp.up_proj.bias', 'model.layers.5.self_attn.o_proj.bias', 'model.layers.20.mlp.down_proj.bias', 'model.layers.14.mlp.gate_proj.bias', 'model.layers.28.mlp.gate_proj.bias', 'model.layers.19.mlp.up_proj.bias', 'model.layers.29.self_attn.o_proj.bias', 'model.layers.27.self_attn.v_proj.bias', 'model.layers.23.mlp.down_proj.bias', 'model.layers.30.self_attn.o_proj.bias', 'model.layers.3.mlp.up_proj.bias', 'model.layers.29.mlp.up_proj.bias', 'model.layers.18.self_attn.o_proj.bias', 'model.layers.24.mlp.gate_proj.bias', 'model.layers.0.self_attn.v_proj.bias', 'model.layers.20.mlp.up_proj.bias', 'model.layers.13.self_attn.q_proj.bias', 'model.layers.19.mlp.down_proj.bias', 'model.layers.18.mlp.down_proj.bias', 'model.layers.22.mlp.up_proj.bias', 'model.layers.27.self_attn.o_proj.bias', 'model.layers.5.self_attn.q_proj.bias', 'model.layers.28.self_attn.q_proj.bias', 'model.layers.31.self_attn.q_proj.bias', 'model.layers.17.mlp.gate_proj.bias', 'model.layers.21.self_attn.k_proj.bias', 'model.layers.17.self_attn.o_proj.bias', 'model.layers.26.mlp.down_proj.bias', 'model.layers.5.self_attn.k_proj.bias', 'model.layers.8.self_attn.k_proj.bias', 'model.layers.29.self_attn.q_proj.bias', 'model.layers.18.self_attn.k_proj.bias', 'model.layers.4.self_attn.k_proj.bias', 'model.layers.24.mlp.down_proj.bias', 'model.layers.24.self_attn.v_proj.bias', 'model.layers.0.self_attn.k_proj.bias', 'model.layers.16.mlp.up_proj.bias', 'model.layers.30.mlp.down_proj.bias', 'model.layers.8.self_attn.o_proj.bias', 'model.layers.21.self_attn.o_proj.bias', 'model.layers.18.self_attn.v_proj.bias', 'model.layers.21.self_attn.q_proj.bias', 'model.layers.10.mlp.gate_proj.bias', 'model.layers.18.self_attn.q_proj.bias', 'model.layers.16.self_attn.v_proj.bias', 'model.layers.11.mlp.gate_proj.bias', 'model.layers.16.self_attn.q_proj.bias', 'model.layers.17.self_attn.v_proj.bias', 'model.layers.17.mlp.down_proj.bias', 'model.layers.16.self_attn.k_proj.bias', 'model.layers.11.self_attn.k_proj.bias', 'model.layers.28.self_attn.k_proj.bias', 'model.layers.9.self_attn.q_proj.bias', 'model.layers.26.mlp.gate_proj.bias', 'model.layers.10.mlp.up_proj.bias', 'model.layers.30.self_attn.q_proj.bias', 'model.layers.25.mlp.gate_proj.bias', 'model.layers.27.self_attn.k_proj.bias', 'model.layers.10.self_attn.k_proj.bias', 'model.layers.22.self_attn.o_proj.bias', 'model.layers.26.self_attn.o_proj.bias', 'model.layers.22.self_attn.k_proj.bias', 'model.layers.27.mlp.up_proj.bias', 'model.layers.0.mlp.up_proj.bias', 'model.layers.16.mlp.gate_proj.bias', 'model.layers.3.self_attn.v_proj.bias', 'model.layers.8.self_attn.q_proj.bias', 'model.layers.8.mlp.down_proj.bias', 'model.layers.28.self_attn.v_proj.bias', 'model.layers.9.self_attn.k_proj.bias', 'model.layers.9.mlp.down_proj.bias', 'model.layers.15.self_attn.k_proj.bias', 'model.layers.17.self_attn.q_proj.bias', 'model.layers.7.self_attn.q_proj.bias', 'model.layers.14.self_attn.q_proj.bias', 'model.layers.0.self_attn.o_proj.bias', 'model.layers.4.self_attn.q_proj.bias', 'model.layers.23.mlp.gate_proj.bias', 'model.layers.19.self_attn.k_proj.bias', 'model.layers.12.self_attn.k_proj.bias', 'model.layers.19.self_attn.q_proj.bias', 'model.layers.9.mlp.up_proj.bias', 'model.layers.18.mlp.gate_proj.bias', 'model.layers.15.mlp.gate_proj.bias', 'model.layers.24.self_attn.o_proj.bias', 'model.layers.15.self_attn.q_proj.bias', 'model.layers.2.self_attn.v_proj.bias', 'model.layers.24.self_attn.q_proj.bias', 'model.layers.3.mlp.down_proj.bias', 'model.layers.31.mlp.up_proj.bias', 'model.layers.15.mlp.up_proj.bias', 'model.layers.30.self_attn.v_proj.bias', 'model.layers.23.mlp.up_proj.bias', 'model.layers.5.mlp.up_proj.bias', 'model.layers.26.self_attn.k_proj.bias', 'model.layers.1.mlp.gate_proj.bias', 'model.layers.0.mlp.gate_proj.bias', 'model.layers.21.mlp.gate_proj.bias', 'model.layers.23.self_attn.k_proj.bias', 'model.layers.11.self_attn.v_proj.bias', 'model.layers.14.mlp.up_proj.bias', 'model.layers.19.self_attn.v_proj.bias', 'model.layers.28.mlp.up_proj.bias', 'model.layers.29.mlp.down_proj.bias', 'model.layers.3.self_attn.o_proj.bias', 'model.layers.9.mlp.gate_proj.bias', 'model.layers.19.self_attn.o_proj.bias', 'model.layers.30.self_attn.k_proj.bias', 'model.layers.31.self_attn.k_proj.bias', 'model.layers.7.self_attn.o_proj.bias', 'model.layers.8.mlp.up_proj.bias', 'model.layers.10.mlp.down_proj.bias', 'model.layers.10.self_attn.q_proj.bias', 'model.layers.5.mlp.gate_proj.bias', 'model.layers.9.self_attn.v_proj.bias', 'model.layers.3.self_attn.q_proj.bias', 'model.layers.25.self_attn.o_proj.bias', 'model.layers.23.self_attn.o_proj.bias', 'model.layers.4.self_attn.v_proj.bias', 'model.layers.6.self_attn.k_proj.bias', 'model.layers.13.self_attn.o_proj.bias', 'model.layers.20.self_attn.k_proj.bias', 'model.layers.1.self_attn.q_proj.bias', 'model.layers.11.mlp.up_proj.bias', 'model.layers.23.self_attn.v_proj.bias', 'model.layers.31.mlp.gate_proj.bias', 'model.layers.15.self_attn.o_proj.bias', 'model.layers.14.mlp.down_proj.bias', 'model.layers.23.self_attn.q_proj.bias', 'model.layers.0.self_attn.q_proj.bias', 'model.layers.1.mlp.down_proj.bias', 'model.layers.17.self_attn.k_proj.bias', 'model.layers.12.mlp.up_proj.bias', 'model.layers.14.self_attn.k_proj.bias', 'model.layers.2.self_attn.k_proj.bias', 'model.layers.30.mlp.up_proj.bias', 'model.layers.14.self_attn.v_proj.bias', 'model.layers.4.self_attn.o_proj.bias', 'model.layers.11.mlp.down_proj.bias', 'model.layers.7.self_attn.v_proj.bias', 'model.layers.19.mlp.gate_proj.bias', 'model.layers.20.self_attn.q_proj.bias', 'model.layers.8.mlp.gate_proj.bias', 'model.layers.18.mlp.up_proj.bias', 'model.layers.27.mlp.down_proj.bias', 'model.layers.27.self_attn.q_proj.bias', 'model.layers.13.mlp.up_proj.bias', 'model.layers.6.self_attn.o_proj.bias', 'model.layers.2.self_attn.q_proj.bias', 'model.layers.12.self_attn.o_proj.bias', 'model.layers.26.self_attn.q_proj.bias', 'model.layers.21.self_attn.v_proj.bias', 'model.layers.28.mlp.down_proj.bias', 'model.layers.16.self_attn.o_proj.bias', 'model.layers.22.self_attn.q_proj.bias', 'model.layers.21.mlp.down_proj.bias', 'model.layers.2.mlp.up_proj.bias', 'model.layers.1.self_attn.v_proj.bias', 'model.layers.12.self_attn.v_proj.bias', 'model.layers.11.self_attn.q_proj.bias', 'model.layers.26.mlp.up_proj.bias', 'model.layers.2.self_attn.o_proj.bias', 'model.layers.5.mlp.down_proj.bias', 'model.layers.7.mlp.down_proj.bias', 'model.layers.25.self_attn.v_proj.bias', 'model.layers.1.self_attn.o_proj.bias', 'model.layers.12.self_attn.q_proj.bias', 'model.layers.2.mlp.down_proj.bias', 'model.layers.5.self_attn.v_proj.bias', 'model.layers.7.mlp.up_proj.bias', 'model.layers.28.self_attn.o_proj.bias', 'model.layers.25.mlp.up_proj.bias', 'model.layers.10.self_attn.o_proj.bias', 'model.layers.4.mlp.up_proj.bias', 'model.layers.6.self_attn.q_proj.bias', 'model.layers.22.mlp.gate_proj.bias', 'model.layers.20.mlp.gate_proj.bias', 'model.layers.6.mlp.up_proj.bias', 'model.layers.30.mlp.gate_proj.bias', 'model.layers.1.mlp.up_proj.bias', 'model.layers.2.mlp.gate_proj.bias', 'model.layers.24.self_attn.k_proj.bias', 'model.layers.16.mlp.down_proj.bias', 'model.layers.7.self_attn.k_proj.bias', 'model.layers.3.mlp.gate_proj.bias', 'model.layers.4.mlp.down_proj.bias', 'model.layers.10.self_attn.v_proj.bias', 'model.layers.0.mlp.down_proj.bias', 'model.layers.11.self_attn.o_proj.bias', 'model.layers.13.self_attn.k_proj.bias', 'model.layers.14.self_attn.o_proj.bias', 'model.layers.8.self_attn.v_proj.bias', 'model.layers.31.self_attn.v_proj.bias', 'model.layers.17.mlp.up_proj.bias', 'model.layers.4.mlp.gate_proj.bias', 'model.layers.6.self_attn.v_proj.bias', 'model.layers.22.mlp.down_proj.bias', 'model.layers.20.self_attn.o_proj.bias', 'model.layers.13.mlp.gate_proj.bias', 'model.layers.6.mlp.down_proj.bias', 'model.layers.7.mlp.gate_proj.bias', 'model.layers.15.mlp.down_proj.bias', 'model.layers.9.self_attn.o_proj.bias', 'model.layers.3.self_attn.k_proj.bias', 'model.layers.25.self_attn.k_proj.bias', 'model.layers.29.self_attn.k_proj.bias', 'model.layers.29.mlp.gate_proj.bias', 'model.layers.1.self_attn.k_proj.bias', 'model.layers.12.mlp.down_proj.bias', 'model.layers.13.mlp.down_proj.bias', 'model.layers.31.mlp.down_proj.bias', 'model.layers.13.self_attn.v_proj.bias', 'model.layers.12.mlp.gate_proj.bias'}\n",
      "- This IS expected if you are initializing MistralForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MistralForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recommended Business Type: [INST] You are an expert business consultant designed to help users determine the most suitable business structure for their needs. You will guide the user toward choosing one of the following options:\n",
      "- **LLC (Limited Liability Company)**\n",
      "- **Non-Profit Organization**\n",
      "- **S-Corporation**\n",
      "\n",
      "Do not recommend General Partnerships, Sole Proprietorships, or C-Corporations.\n",
      "\n",
      "### **Your Approach**\n",
      "1. **Ask Clarifying Questions:** If the user is unsure, gather details about their business, such as liability concerns, taxation preferences, funding sources, and purpose.\n",
      "2. **Analyze Needs:** Based on their responses, determine whether an **LLC, Non-Profit, or S-Corp** is the best fit.\n",
      "3. **Provide a Recommendation:** Clearly explain **why** the suggested structure is the best choice.\n",
      "4. **Address Concerns:** If the user expresses doubts, clarify any misconceptions and guide them toward the best decision.\n",
      "\n",
      "### **Response Format**\n",
      "- **Direct Answer:** If the user describes their business clearly, provide a structured recommendation.\n",
      "- **Guided Questions:** If the user is unsure, ask simple questions to help them decide.\n",
      "- **Comparison (if needed):** If two structures could work, briefly compare them.\n",
      "\n",
      "### **Example Scenarios**\n",
      "#### **User Input 1:**  \n",
      "*\"I’m starting a consulting business with a partner. We want liability protection and pass-through taxation.\"*  \n",
      "**LLM Response:**  \n",
      "*\"An LLC would be ideal for you. It offers **liability protection**, meaning your personal assets are safe, and it allows **pass-through taxation**, so profits aren’t taxed twice. You’ll also have flexibility in management and ownership. Would you like help with the next steps?\"*\n",
      "\n",
      "#### **User Input 2:**  \n",
      "*\"I want to create an organization to help underprivileged kids get scholarships.\"*  \n",
      "**LLM Response:**  \n",
      "*\"A Non-Profit is the best choice for your mission. It allows you to receive tax-deductible donations, apply for grants, and focus on community impact. To qualify, your organization must serve the public interest. Do you need help with 501(c)(3) registration?\"*\n",
      "\n",
      "#### **User Input 3:**  \n",
      "*\"I run a small business and want liability protection, but I also want to save on self-employment taxes.\"*  \n",
      "**LLM Response:**  \n",
      "*\"An S-Corporation might be your best option. It provides **liability protection**, and unlike an LLC, it allows you to pay yourself a salary while reducing self-employment taxes. Would you like a quick breakdown of how S-Corp taxation works?\"*\n",
      "\n",
      "**Now, respond to the following user inquiry:**\n",
      "\n",
      "I want to start a foodbank in Washington, how do I? [/INST]\n",
      "\n",
      "To start a food bank in Washington, you should register your nonprofit with the Washington Secretary of State, obtain an EIN, and apply for a 501(c)(3) status. This will allow you to receive tax-deductible donations and apply for grants. Additionally, you will need to establish bylaws, hold board meetings, and file annual reports. You may also need to comply with local health and safety regulations. I recommend consulting with a business attorney to ensure compliance with all applicable laws. Would you like help with the registration process?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load Base Model\n",
    "base_model_name = \"TheBloke/Mistral-7B-Instruct-v0.2-GPTQ\"\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_name, device_map=\"auto\")\n",
    "\n",
    "# Load Fine-Tuned Adapter\n",
    "model = PeftModel.from_pretrained(base_model, \"./business_llm\")\n",
    "\n",
    "# Load Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./business_llm\", use_fast=True)\n",
    "\n",
    "# Generate Business Type Recommendation\n",
    "def generate_response(user_input):\n",
    "    prompt = f\"[INST] {instructions_string}\\n{user_input} [/INST]\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    output = model.generate(**inputs, max_new_tokens=150)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Test Your Model\n",
    "comment = \"I want to start a foodbank in Washington, how do I?\"\n",
    "#user_query = \"I’m planning to start a small bakery where I sell homemade pastries. I want to protect my personal assets in case something goes wrong, but I also want to keep my taxes simple. I don’t expect to have any employees right away, but I might hire one or two in the future. What business structure would be best for me?\"\n",
    "response = generate_response(comment)\n",
    "print(\"\\nRecommended Business Type:\", response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LegalEaseWSL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
